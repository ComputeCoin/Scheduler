\documentclass{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{multirow}
 \usepackage[
	backend=biber,
	style=ieee,
]{biblatex}
\usepackage{csquotes}
\addbibresource{references.bib}

\newenvironment{notation}
{
    \newpage

    \paragraph{\Huge Notation}
    \begin{flushleft}
}
{
    \end{flushleft}
}

\newcommand{\notate}[2]{$#1 \hspace{0.35in} \mbox{#2}$}

% Theorems
\theoremstyle{definition}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{remark}

\newtheorem*{remark}{Remark}
\newtheorem*{theorem}{Theorem}


% Commands
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\reqfunc}[2]{#1:#2\rightarrow\mathbb{B}}
\newcommand{\reqop}[2]{\mathbf{Req}_{#1}(#2)}

\newcommand{\Path}[2]{\mathbf{Path}^{#2}_{#1}}
\newcommand{\pathall}[1]{\mathbf{Path}_{#1}}
\newcommand{\cyclepath}[1]{\mathbf{Cycle}_{#1}}

\newcommand{\labeledarrow}[1]{\stackrel{#1}{\rightarrow}}

\newcommand{\addpic}[1]{\includegraphics[width=1.0\textwidth]{#1}}

\newcommand{\setbuild}[3]
{
	\{\hspace{0.05in}
	#1 \in #2 \hspace{0.05in}
	| \hspace{0.05in}#3\hspace{0.05in}
	\}
}

\makeatletter
\@addtoreset{section}{part}
\makeatother

\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0in}

% Preamble
\title{Scala Job Scheduler}
\date{March 2018}

% --------------------------------------------------------------------------
\begin{document}

	% Title page
	\pagenumbering{gobble}
	\maketitle

	%Abstract
	\begin{abstract}
		We present a novel scheduling architecture PANDA (Policy Advisor Network and Decision Architecture) using a multi-agent reinforcement learning model, specially designed for cloud environments with dynamic allocation of computing resources for inhomogeneous consumer specifications. The architecture consists of a policy advisor network (PAN), designed with a hierarchical topology yielded from Bayesian hierarchical clustering analysis on consumer specifications, a catch-release-wait actor network used by scheduling agents  which are trained using reinforcement learning methods, which result in many advantages over traditional schedulers.
	\end{abstract}

	% Begin main document
	\newpage
	\pagenumbering{arabic}

	\addcontentsline{toc}{section}{Introduction}

	\section*{Introduction}

	The aim of this project is to design an optimal scheduling algorithm for a scalable computing cloud, where computing resources are dynamically allocated to meet the demands of an inhomogeneous set of consumers. Resources are not uniformly distributed, geographically or otherwise, as the nodes comprising the cloud are of variable type and processing power. Clients will submit job specifications (indicating the number and type of cores, ideal network topology, arrival time, required run time, memory size, etc.) to the scheduler, which should designate a time to run and a cluster of nodes that adheres to the specification. The algorithm should minimize expected average total time in system for all users, while maintaining fairness between jobs that place similar demands on the system. The algorithm should also be capable of adapting to and achieving optimal scheduling in highly variable cloud environments, while reducing the number of accounted metrics for scheduling optimization.

	In such a dynamic, diverse system (given the sheer number of factors to account	for), traditional static scheduling algorithms such as linear programming can often be nullified by rapidly changing and sometimes unreliable resources. Therefore, we elected to confront the problem with a reinforcement learning algorithm specifically adapted to this and similar environments. The algorithm is highly dependent on the system's partitioning into measurable (numerically describable) components, and typifying these components for efficient processing - thus we focused on separability and producing quantifiable descriptors of the system. What follows is a detailed specification of the resultant scheduling paradigm: first of the reinforcement model PANDA (Policy Advisor Network and Decision Architecture), and then training of this model and further discussions.


	%\section{Previous Work}
	
	
	\section{Preliminary Data Processing}
	
	In the interest of improving efficiency and accelerating convergence of the reinforcement model, aspects of the environment are grouped utilizing a probabilistic approach to agglomerative hierarchical clustering called Bayesian hierarchical clustering (Heller and Ghahramani 2005). Using this method, consumers, resources, and states are classified into types, each representable by a numeric label (e.g. 1, 2). After classification, consumer specification parameters and consumer types are fed into a policy advisor network, which then outputs policy parameters for a scheduling agent to use for the duration of their search. Resource and state types are reserved for components of the agent decision model, where they can greatly improve algorithmic performance simply by reducing their respective spaces to tractable sizes.

	\section{Policy Advisor Network and Decision Architecture (PANDA)}

	This proposed architecture can handle both dynamic agent population and state space when processing diverse systems. It can also handle static resource space by default. This architecture observes the total time of a user being in system as the only metric for scheduling optimization, by reducing other metrics to be about time. From this perspective, the PANDA is simpler and more efficient than traditional static schedulers. It can counter greedy scheduling algorithm to a more extensive degree, which in return gives novel insight into general scheduling processes.

		\subsection{Overview}

		The diagram below shows the complete PANDA model, which represents one time step of the entire algorithm. Each parameter class is represented using a color residing in the box labeled agents. The square box represents a specification parameter class and there are a set of agents that exist within that class at any one time step.

		\addpic{figures/general_diagram.png}

		Upon submission by a user, a consumer enters the scheduling process and submits its specifications as a parameter vector, which is given to the policy advisor as input. The policy advisor produces a policy parameter vector which is then used as the weights of the actor network. The actor network is then used as the mechanism that the representing scheduling agent will sample from in order to perform actions within the system. In the end, the consumer leaves the scheduling process by consuming (running on) assigned resources. Once finished, the consumer gives feedback to the scheduler for further training improvement.

		There are three levels of action under analysis, the agent action, the class action, and the system action. An agent action is produced by the actor network of a scheduling agent, a class action is a list of agent actions within the same class, and a system action is a list of class actions submitted to the system, which is referred to in the diagram as the joint action of all the agents.

		\subsection{Model}

		Given the nature of the dynamic and diverse system, the rigidness of other traditional scheduling solutions would be a critical problem in the efficiency and complexity of the scheduling. A reinforcement learning approach makes the system adaptable and flexible to changing conditions of the environment, which is very desirable. This model hopes to be able to give insight into multi-agent problems, as well as how to correct the relatively common sub-optimal solutions of greedy algorithms. Additionally the model hopes to show how reinforcement learning can be useful in combinatorial problem solving.

			\subsubsection{Policy Advisor Network (PAN)}

			The policy advisor is the mechanism which consumers utilize to encode their specification into their representative scheduling agents. These agents then use the output given by the policy advisor corresponding to their class in order to parameterize their policy. The policy advisor is defined as follows.

			\begin{definition}
				The \emph{policy advisor} is a function $\mathcal{P}: \Gamma \rightarrow \Theta$, where $\Gamma$ is the specification parameter space and $\Theta$ is the policy parameter space.
			\end{definition}

			\begin{remark}
				The policy advisor is a neural network, $\mathcal{P}_{\mathbf{w}}(\gamma)$, initialized with a hierarchical topology. This is done with respect to the expected clusters emerging in $\Gamma$.
			\end{remark}


				\paragraph{Topology}

				The diagram below shows a simplified PAN topology. The colors represent the different classes of the specification parameter space. The numeric labels are indicative of the specification parameter vectors.

				\addpic{figures/advisor_topology.png}

		\subsubsection{Agent Model}
		
			\paragraph{Agents and Actor Networks}

			Each consumer that enters the system is assigned an agent, which operates according to the policy output by the advisor network. The goal of an agent is to maximize reward collected by the PAN by obtaining the optimal set of resources that satisfies the consumer's specification. Resource collection occurs through the following iterative process, executed on each time step:
			
			\addpic{figures/actor_network}

			\begin{enumerate}

				\item Given the state of the system, its policy parameters, and its assigned specification, each agent selects an action to take based on a stochastic decision model. The action is a composite structure of atomic action units, and will be described in detail later in this section.

				\item The action is submitted to the system supervisor, which regulates large-scale behavior and controls the supply of reward to the PAN. Actions received from all agents are then interpreted and applied to the environment, potentially inducing a state transition (between state categories, as states are clustered in the same manner as consumer specifications).

				\item Data on all consumers are updated, and reward is calculated and delivered to the PAN. Policies are revised accordingly, and a form of supervised learning is employed to reconfigure the network itself to better conform to the new policies.

			\end{enumerate}

			Agent decision-making is driven by a neural network known as an actor network, uniquely configured to meet the demands of the agent's consumer. Each actor network is a function $\func{\alpha}{S \times A}{\mathbb{R}}$, such that $\alpha_\theta(a | s)$ is a measure of the expected value of the action $a$ to the consumer. The parameters output by the PAN serve as weights for the actor network, and as such are the PANDA's means of manipulating agent activity. Here it is important to note that this algorithm differs from traditional reinforcement learning algorithms in that reward is dispensed to a PAN rather than the reinforcement agents. The weights on the agent's actor networks themselves are not trained - instead, training is concentrated on the PAN that generates the weights from consumer specifications. In fact, agents and their networks have finite lifetimes, as they are disposed of upon becoming unemployed, with collected data extracted and incorporated into the next iteration of the PAN.

			\begin{remark}
				An agent is said to be \emph{unemployed} when the consumer to which it has been assigned has departed from the system. As agents are specifically designed to cater to a particular consumer, unemployed agents have minimal value to the scheduler beyond the information they have gathered over their lifetimes.
			\end{remark}

			\paragraph{Partial Observation and Attention Mechanism}
			
			In an effort to reduce the time complexity of the agent decision process, agent observation is limited to subset of viable resources rather than the entire resource space. This form of agent perception is not unlike that of humans, who tend to fixate on objects of interest and filter out irrelevant features of their environment. To mimic a human-like attention mechanism, the agents thus apply an appropriately named filter function $\mathcal{F}: \mathbf{Set} \rightarrow \mathbf{Set}$ to sets of resources as they become available for consumption. Mathematically speaking, $\mathcal{F}$ maps a set of resources $X$ to some $X' \subseteq X$, such that $X'$ contains only those elements of $X$ whose expected value to the consumer eclipses a certain threshold. 
			
			The implementation of this function hinges on how the relative value of a resource may be estimated without compromising efficiency or relying on static numerical thresholds, which could compromise performance when operating within a dynamic environment. A potential solution employs the categorical representation of resources by assigning a weight to each resource type based on its conformity to the specification. As each cluster occupies some location in N-dimensional space, given by its center point, a weighted average of the type locations may be computed to estimate the ideal location for a resource given the consumer's specification. Resources within a certain distance limit (perhaps some function of the variance in type location) pass the filter, and are integrated into the agent's pool of observed resources.

			\paragraph{Agent Actions}

			At the end of each time step, agents submit an action to the system supervisor consisting of three elementary action units. These action units will be performed in the order listed below upon execution of the action. The basic action units are as follows:
			
			\addpic{figures/agent_action.png}
			
			\begin{enumerate}
				\item Catch - the agent acquires an available resource to be assigned to its consumer. Caught resources are held until consumed or a \emph{release} action is taken. Catch requests are submitted as a Boolean vector, with each component corresponding to an item in the pool of observed resources.
				\item Release - the agent returns a held resource to the pool of available resources. Release requests are also submitted as a Boolean vector, with each component corresponding to an item in the pool of held resources.
				\item Wait - the agent delays consumption of resources until the subsequent time step. Wait requests are submitted as a Boolean scalar, as only one wait action may be performed per time step.
			\end{enumerate}
		
			

			The actor network output is mapped to a Boolean vector by applying the softmax function and sampling from the resultant categorical distribution.

		\subsubsection{System Supervisor}

		Agent decisions are submitted as actions to a system supervisor, which resolves collisions between catch requests\footnote{Collision resolution methods currently being considered are (a) allocating the resources in question on a first come, first served basis, and (b) allocating them on the basis of need, determined from the output of the agent's actor networks.}. Once conflicts are resolved, the agent actions are executed in order of submission. The supervisor then initiates consumption of collected resources, excluding those held by an agent whose action included a wait operation. 
		
		The supervisor also tracks the progression of each specification through the system, and distributes reward to the respective agents according to a function of the collected data and potential consumer feedback. In the instance of the scalable cloud, reward will be computed upon completion of the job, at which point the total time spent in the system (the sum of wait time and run time) and any consumer feedback will be available and may be factored into the calculation. To maximize fairness, longer wait times would be permissible for jobs that placed larger demands on the system (in core hours). Total time is the main factor to consider, as minimizing wait time will maximize system utilization, while reduced run time is a result of optimized resource selection. Both times would be normalized relative to expected values derived from the specification parameters and the state of the system.

	\section{Training}

		\subsection{Model Training}
		% Question:
		% 	    1.  How exactly do you plan on training the model?
		% 	    2.  How long is this training going to take?
		% 	    3.  How do you plan on mitigating the known set backs?
		
		As was previously mentioned, the only metric that will be observed is the total time a consumer is in the system. With this in mind, we will first give a first attempt at a definition of what constitutes a sufficient scheduling policy. Let $c_{n}$ represent the $n$th consumer leaving system. Firstly, we will observe a stochastic process, $\mathcal{D} = \{(T_{n}, \delta_{n}, \mu_{n}); n \in \mathbb{N}\}$, where $T_{n}$ is the time between $c_{n}$ and $c_{n-1}$ times of occurence, $\delta_{n}$ is the total time consumer $c_{n}$ was in the system, and $\mu_{n}$ is the utility of consumer $c_{n}$.
	
		\begin{definition}
			Let $\mathcal{D} = \{(T_{n}, \delta_{n}, \mu_{n}); n \in \mathbb{N}\}$ be a stochastic process and $\mathbf{c} = (c_{1}, \ldots ,c_{n})$ be a sequence of consumers that have left the system. A scheduling policy is said to be sufficient if
			\[
			S_{n} \in \left[\hspace{0.1in}\max_{i \in \underline{n}}\{\delta_{i}\}, \hspace{0.1in} \sum_{i \in \underline{n}}{\delta_{i}}\right] \subseteq \mathbb{R}_{+}
			\] 
			where $S_{n} = \sum_{i=1}^{n}{T_{i}}$.
		\end{definition}
		
		\begin{remark}
			The distance of $S_{n}$ from the lower and upper bound represent the degree of parallel uses of the resources, the lower bound being embarassingly parallel and the upper bound being strictly sequential.
		\end{remark}
		The intuition following this definition is that a sufficient scheduling policy will effectively , depending on the resources, force the sum of interdeparture times to fall within these bounds. If $S_{n}$ falls above the upper bound then the scheduling policy was \emph{inefficient}.
	
		The data that will be collected on the system are those corresponding to the arrival, scheduling, and departure process. The data set will be of the form
		\[
		T = \{(\gamma_{i}, W_{i},\vec{t}_{i}, \vec{\delta}_{i}, \mathcal{R}_{i}, r_{i})\}_{i = 1}^{n} 
		\]
		where $\gamma_{i}$ is a specification parameter vector, $W_{i}$ are the weights used for $\gamma_{i}$, $\vec{t}_{i} = (t_{1}, t_{2}) \in \mathbb{R}_{+}^{2}$ such that $t_{1}$ is the arrival time and $t_{2}$ is the departure time of $\gamma_{i}$, $\vec{\delta}_{i} = (\delta_{w}, \delta_{u}) \in \mathbb{R}_{+}^{2}$ such that $\delta_{w}$ is the wait time and $\delta_{u}$ is the use time of $\gamma_{i}$, $\mathcal{R}_{i}$ is the set of resources assigned to $\gamma_{i}$, and $r_{i}$ is the reward given to $\gamma_{i}$. 
		
		From this data set we will be able to determine the data for the arrival, scheduling, departure processes. Additionally, any information required by the PANDA will be derived from this data set. 
		
		The model will be trained using the data collected on the agents in the system, using the rewards and updated policy parameters from each individual agent to update the weights of the PAN. Each agent will collect reward at the end of each episode in the system (when a successful scheduling has occurred). This training will most likely under go a slow convergence considering the parameters being trained are the weights of the actor networks of the agents. Currently, we are constructing methods for translating rewards to other specification and policy parameters by using measure-preserving transformations. This method is explored as a way for overcoming the problem of learning how to distribute rewards for different policy parameters. This will hopefully lead to faster convergence, with respect to the PAN.

	\section{Discussion}
	
		The major advantages of using PANDA for job scheduling is that it allows to schedule jobs within a dynamic environment, allows for more user autonomy, and looks at the problem from a meta standpoint as opposed to an individual basis.  This all allows for a more streamline scheduling experience that allows for a more cohesive system that relies on less metrics than a typical static scheduler.
		
		The setbacks for this approach include focusing too much on the big picture, or metadata, instead of looking at individual cases to better substantiate useful data, and the idea of letting users as well as the policy advisor handle fairness may lead to potential conflicts between consumers.
		
		The plan is to collect consistent analysis of data that comes in with each consumer and create a relationship function between the parameters so we can extrapolate where conflicts occur, hence being able to push policy advisors to make certain decisions based on expectations of recent data.
		
		A general multitude of simulations will be run in order to get an idea of what possible conflicts may arise with our approach to this problem, allowing us to fine tune individual issues, which can allow us to handle problems that arise in the metadata before even receiving said data.

		\paragraph{Hybrid Approach} 
		
		In order to integrate PANDA into a real-time system we will gradually transition from a static model. We will implement a cake-cutting algorithm that will use a discrete envy-free protocol, equipped for handling any number of agents. 
		
		\begin{remark}
			This approach will be dealt with on a cautionary level because the integrity of scheduling data will be affected as different scheduling algorithms are used. We will take this as an opportunity to gather large amounts of data that exceed the currently available data in literature and given out by other companies (e.g. Google Cloud Platform and Amazon Web Services).
		\end{remark}
	
	\section{Future Work}
	
	In future work, we will be establishing more formal and concrete definitions on scheduling policies concerning generic resources. We will be exploring this by utilizing literature on measure-preserving dynamical systems, decentralized partially observable Markov decision processes, and competitive/cooperative multi-agent systems. Additionally, we will be observing and analyzing the real-time data we recieve from the system, in order to guide the future of the architecture in the right direction. We will also be keeping the integrity of the data as our highest priority, as for it to have no dependence on the scheduling system that is used. This choice should only affect observations such as interdeparture times and the average total time in the system for a consumer.

	%\newpage
	
	%\cite{carastan2017obtaining}

	%\printbibliography

\end{document}

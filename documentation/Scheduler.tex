\documentclass{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{multirow}

\newenvironment{notation}
{
    \newpage
    
    \paragraph{\Huge Notation}
    \begin{flushleft}
}
{
    \end{flushleft}
}

\newcommand{\notate}[2]{$#1 \hspace{0.35in} \mbox{#2}$}

% Theorems
\theoremstyle{definition}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{remark}

\newtheorem*{remark}{Remark}
\newtheorem*{theorem}{Theorem}

% Commands
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\reqfunc}[2]{#1:#2\rightarrow\mathbb{B}}
\newcommand{\reqop}[2]{\mathbf{Req}_{#1}(#2)}

\newcommand{\path}[2]{\mathbf{Path}^{#2}_{#1}}
\newcommand{\pathall}[1]{\mathbf{Path}_{#1}}
\newcommand{\cyclepath}[1]{\mathbf{Cycle}_{#1}}

\newcommand{\labeledarrow}[1]{\stackrel{#1}{\rightarrow}}

\newcommand{\addpic}[1]{\includegraphics[width=1.0\textwidth]{#1}}

\newcommand{\setbuild}[3]
{
	\{\hspace{0.05in} 
	#1 \in #2 \hspace{0.05in} 
	| \hspace{0.05in}#3\hspace{0.05in}
	\}	
}

\makeatletter
\@addtoreset{section}{part}
\makeatother

% Preamble
\title{Scala Job Scheduler}
\date{March 2018}

% --------------------------------------------------------------------------
\begin{document}
	
	% Title page
	\pagenumbering{gobble}
	\maketitle
	
	%Abstract
	\begin{abstract}
		We design a novel scheduling architecture PANDA (Policy Advisor Network and Decision Architecture) using
		multi-agent reinforcement learning model, specially for cloud environment with dynamically allocated computing
		resources and inhomogeneous requirements from consumers. This architecture consists of policy advisor with
		Bayesian hierarchical clustering, catch-release-wait agent action mechanism and reinforcement training,
		which lead to its many advantages over traditional schedulers.
	\end{abstract}
	
	% Table of contents
	\newpage
	\tableofcontents
	
	% Begin main document
	\newpage
	\pagenumbering{arabic}
	
	\addcontentsline{toc}{section}{Introduction}

	\section*{Introduction}

	\indent
	
	The aim of this project is to design an optimal scheduling algorithm for a scalable computing cloud, where computing resources are dynamically allocated to meet the diverse demands of an inhomogeneous set of consumers. Resources are not uniformly distributed, geographically or otherwise, as the nodes comprising the cloud are of variable type	and processing power. Clients will submit job specifications (indicating the number and type of cores, ideal network topology, arrival time, required run time, memory size, etc.) to the scheduler, which should designate a time to run and a cluster of nodes that adheres to the specification. The algorithm should minimize expected average total time in system for all users, while maintaining fairness between jobs that place similar demands on the system. The algorithm should also be able to achieve	optimal scheduling and adapting under rapidly changing cloud environment conditions, while reducing number of accounted metrics for scheduling optimization.
	
	In such a dynamic, diverse system (given the sheer number of factors to account	for), traditional static scheduling algorithms, for example, linear programming, can often be nullified by rapidly changing and sometimes unreliable resources. Therefore we elected to confront the problem with a reinforcement learning algorithm built on the aforementioned model.	The algorithm is highly dependent on the system's partitioning into measurable (numerically describable) components, and typifying these components for efficient processing - thus the focus on separability and producing quantifiable descriptors of the system. What follows is a detailed specification of the resultant scheduling paradigm: first of the reinforcement model PANDA (Policy Advisor Network and Decision Architecture), and then training of this model and further discussions.
	
	
	\section{Previous Work}
	
	\section{Policy Advisor Network and Decision Architecture (PANDA)}
	
	This proposed reinforcement architecture can handle both dynamic agent population and state space when processing diverse system. It can also handle static resource space by default. This architecture observes the total time of a user being in system as the only metric for scheduling optimization, by reducing other metrics to be about time in the first place. From this perspective, the PANDA architecture is simpler and more efficient than traditional static schedulers. It can counter greedy scheduling algorithm to a more extensive degree, which in return gives novel insight of general scheduling processes.
		
		\subsection{Overview}
	
		The diagram below shows the complete PANDA model, which represents one time step of the entire algorithm. Each parameter class is represented using a color residing in the box labeled agents. The square box represents a specification parameter class and there are a set of agents that exist within that class at any one time step.
	
		\addpic{figures/general_diagram.png}
	
		Upon submission by user, a consumer (user) enters the scheduling process and submits its specifications as a parameter vector, which is given to the policy advisor as input. The policy advisor produces a policy parameter vector which are then used as the weights of the actor network. The actor network is then used as the mechanism that the representing scheduling agent will sample from in order to perform actions within the system. In the end, the consumer leaves the scheduling process by starting consuming (running on) assigned resources. Once finished, the consumer gives feedback to the scheduler for further training improvement.

		There are three levels of action that are looked at, the agent action, the class action, and the system action. An agent action is produced by the actor network of an scheduling agent, a list of agent actions within the same class is a class action, and a list of class actions submited to the system is a system action, which is referred to in the diagram as the joint action of all the agents.		

		\subsection{Model}
		
		Given the nature of the dynamic and diverse system, the rigidness of other traditional scheduling solutions would be a critical problem in the efficiency and complexity of the scheduling. A reinforcement learning approach makes the system adaptable and flexible to changing conditions of the environment, which is very desirable. This model hopes to be able to give insight into multi-agent problems, as well as how to correct the relatively common sub-optimal solutions of greedy algorithms. Additionally the model hopes to show how reinforcement learning can be used usefully in combinatorial problem solving.
		
			\subsubsection{Policy Advisor Network}
	
			The policy advisor is the mechanism which consumers utilize to encode their specification into their representative scheduling agents. These agents then use the output given by the policy advisor corresponding to their class in order to parameterize their policy. The policy advisor is defined as follows.
		
			\begin{definition}
				The \emph{policy advisor} is function $\mathcal{P}: \Gamma \rightarrow \Theta$, where $\Gamma$ is the specification parameter space and $\Theta$ is the policy parameter space.
			\end{definition}
		
			\begin{remark}
				The policy advisor is a neural network, $\mathcal{P}_{\mathbf{w}}(\gamma)$, initialized with a hierarchal topology. This is done with respect to the expected clusters emerging in $\Gamma$.
			\end{remark}
				

				\paragraph{Topology}
				
				The diagram below shows a simplified advisor network topology. The colors represent the different classes of specification parameter vectors. The numeric labels are indicative of the specification paramter vectors.
				
				\addpic{figures/advisor_topology.jpeg}

		\subsubsection{Agent Model}
		
			\paragraph{Partial Observation and Attention Mechanism}
	
			\paragraph{Catch-Release-Wait Mechanism (CRW)}
	
			At the end of each time step, agents submit a \emph{joint action} to the system supervisor consisting of a sequence of elementary action units. These action units will be performed in order upon execution of the joint action. The basic action units are as follows:
	
			\begin{enumerate}
				\item Catch - the agent acquires an available resource to be assigned to its consumer. Caught resources are held until consumed or a \emph{release} action is taken.
				\item Release - the agent returns a held resource to the pool of available resources.
				\item Wait - the agent delays consumption of resources until the subsequent time step. Wait actions may only occur once per joint action, and only as the last action in the sequence.
			\end{enumerate}
		
		\subsubsection{System Supervisor}
		
		Agent decisions are submitted as joint actions to a system supervisor, which resolves collisions between catch requests so as to maximize the net gain of all agents in the system. The gain function will be analytically constructed to provide functionality in the short-term, but will be replaced with a trained version once the reinforcement model has converged sufficiently for effective resolution. The supervisor then initiates consumption of collected resources, excluding those held by an agent whose joint action concluded with a wait operation.
	
		The supervisor also tracks the progression of each specification through the system, and distributes reward to the respective agents according to a function of the collected data and potential consumer feedback. In the instance of the scalable cloud, reward will be computed upon completion of the job, at which point the total time spent in the system (the sum of wait time and run time) and any consumer feedback will be available and may be factored into the calculation. To maximize fairness, longer wait times would be permissible for jobs that placed larger demands on the system (in cores or core hours). Total time is the main factor to consider, as minimizing wait time will maximize system utilization, while reduced run time is a result of optimized resource selection. Both times would be normalized relative to expected values derived from the specification parameters and the state of the system.


	\section{Training}
	
		\subsection{Pre-Training}
		
		Using Bayesian hierarchical clustering, consumers are classified into a certain consumer type represented by a numeric label (e.g. 1, 2). After classification, the consumer then enters their specification parameters and consumer type into a policy advisor which then outputs policy parameters for a scheduling agent to use, for the duration of their search.
	
		\subsection{Model Training}
		% Question:
		% 	    1.  How exactly do you plan on training the model?
		% 	    2.  How long is this training going to take?
		% 	    3.  How do you plan on mitigating the known set backs?
		
		The model will be trained using the data collected on the agents in the system and then using the rewards and updated policy parameters from each individual agent to update the weights of the PAN. Each agent will collect reward at the end of each episode in the system (when a successful scheduling has occured). This training will most likely go under a slow convergence considering the parameters being trained are the weights of Currently, we are constructing methods for translating rewards to other specification and policy parameters through by using linear transformations. This method is explored as a way for overcoming the problem of learning how to distribute rewards for different policy parameters. This will hopefully lead to faster convergence, with respect to the PAN.
		
	\section{Discussion}
	
		\paragraph{Hybrid Approach}

	\newpage

	\begin{notation}
		\notate{\gamma}{specification parameter}\\
		\notate{\Gamma}{specification parameter space}\\
		\notate{C}{a subset of consumers}\\
		\notate{v}{a service}\\
		\notate{V}{Set of services}\\
		\notate{\rho(X)}{requirement function}\\
		\notate{\reqop{\rho}{X}}{requirement operator with parameter set X}\\
        \notate{\wedge}{and (joint)}\\
        \notate{\cap}{intersection}\\
        \notate{\psi(X)}{inspection function}\\
        \notate{\phi}{acceptance requirement}\\
		\notate{a}{an arriving process}\\
		\notate{A,G}{assignment actions}\\
		\notate{g}{an assignment process}\\
		\notate{d}{a departure process}\\
		\notate{D}{a departure action}\\
        \notate{G}{a graph with vertices V, and Edges E}\\
        \notate{\psi(X)}{inspection function}\\
        \notate{\path{G}{(n)}}{a path in graph G of length n}\\
        \notate{\sqcup}{disjoint union}\\
        \notate{\cyclepath{G}(v)}{cycle in graph G from vertex v to vertex v}\\
        \notate{\mathcal{P}(\gamma)}{policy advisor}
     \end{notation}

	\newpage
	
\end{document}
